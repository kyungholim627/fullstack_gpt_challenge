{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Designate variables\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages= True,\n",
    ")\n",
    "\n",
    "loader = TextLoader(r\"./files/chapter_3.txt\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./cache/\")\n",
    "\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir    \n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# List of docs generation\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim\n",
    "    ------------------\n",
    "    {portion}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# Merge list of docs\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "        \"portion\": doc.page_content,\n",
    "        \"question\": question\n",
    "    }).content for doc in documents)\n",
    "\n",
    "map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "# Final document | prompt | llm\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    Give the following extracted parts of a long document and a question, create a final answer.\n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up the answer.\n",
    "    ---------\n",
    "    {context}\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(input):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | RunnablePassthrough.assign(chat_history=load_memory) | final_prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result=chain.invoke(question)\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content}\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"Is Aaronson guilty?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
